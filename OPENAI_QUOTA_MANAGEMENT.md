# Gestion du Quota OpenAI API

## üî¥ Erreur 429 - Quota d√©pass√©

Si vous recevez une erreur 429, cela signifie que votre quota OpenAI API a √©t√© d√©pass√©.

## ‚úÖ Solutions imm√©diates

### 1. V√©rifier et g√©rer le budget

1. **Allez sur la page Limits** :
   - https://platform.openai.com/org/limits
   - Connectez-vous avec votre compte OpenAI

2. **V√©rifiez votre budget d'organisation** :
   - Regardez la section **"Organization budget"**
   - Vous verrez : `$X.XX / $Y.YY` (montant utilis√© / budget total)
   - La date de r√©initialisation est affich√©e (ex: "Resets in 16 days")

3. **Configurez des alertes** :
   - Activez les alertes √† 80% et 100% d'utilisation
   - Cliquez sur "Add alert" pour cr√©er des alertes personnalis√©es
   - Vous recevrez des notifications par email quand vous approchez de vos limites

4. **Modifiez le budget si n√©cessaire** :
   - Cliquez sur "Edit budget" pour augmenter votre budget
   - Ajoutez un moyen de paiement si n√©cessaire
   - Les cr√©dits sont g√©n√©ralement ajout√©s imm√©diatement

### 2. V√©rifier les limites d'utilisation (RPM, RPD, TPM)

#### M√©thode 1 : Via le Dashboard OpenAI (Recommand√©)

1. **Allez sur le dashboard OpenAI** :
   - https://platform.openai.com/
   - Connectez-vous avec votre compte

2. **Acc√©dez √† la page Limits** :
   - Dans la barre lat√©rale gauche, cliquez sur **"Billing"**
   - Puis s√©lectionnez **"Limits"** (surlign√© dans le menu)
   - Ou acc√©dez directement via : https://platform.openai.com/org/limits

3. **Sur la page Limits, vous verrez** :
   - **Organization budget** : Votre budget actuel et la date de r√©initialisation
   - **Usage alerts** : Alertes configur√©es (80%, 100%)
   - **Rate limits table** : Tableau d√©taill√© avec toutes les limites par mod√®le

4. **Dans le tableau des limites, vous trouverez** :
   - **TPM (Tokens Per Minute)** : Limite de tokens par minute pour chaque mod√®le
   - **RPM (Requests Per Minute)** : Limite de requ√™tes par minute
   - **TPD (Tokens Per Day)** : Limite de tokens par jour (pour Batch Queue)
   - **Note** : Les limites peuvent varier selon la version sp√©cifique du mod√®le (cliquez pour d√©velopper le tableau)

#### M√©thode 2 : Via l'API (programmatique)

Vous pouvez v√©rifier les limites en regardant les headers de r√©ponse de l'API :

```typescript
// Exemple de v√©rification des limites
const response = await axios.post(openaiApiUrl, data, { headers });
const limits = {
  rpm: response.headers['x-ratelimit-limit-requests'],
  tpm: response.headers['x-ratelimit-limit-tokens'],
  remaining: response.headers['x-ratelimit-remaining-requests'],
  reset: response.headers['x-ratelimit-reset-requests']
};
```

#### Limites par d√©faut selon le mod√®le

**Note importante** : Les limites varient selon votre "Usage tier" (niveau d'utilisation). V√©rifiez toujours votre page Limits pour voir vos limites exactes.

**Exemples de limites (Usage tier 1)** :

**Pour `gpt-4.1`** :
- **TPM** : 30,000 tokens par minute
- **RPM** : 500 requ√™tes par minute
- **TPD** : 900,000 tokens par jour (Batch Queue)

**Pour `gpt-5.1`** :
- **TPM** : 30,000 tokens par minute
- **RPM** : 500 requ√™tes par minute
- **TPD** : 900,000 tokens par jour (Batch Queue)

**Pour `gpt-5-mini`** :
- **TPM** : 500,000 tokens par minute
- **RPM** : 500 requ√™tes par minute
- **TPD** : 5,000,000 tokens par jour (Batch Queue)

**Pour `gpt-5-nano`** :
- **TPM** : 200,000 tokens par minute
- **RPM** : 500 requ√™tes par minute
- **TPD** : 2,000,000 tokens par jour (Batch Queue)

**Pour `gpt-3.5-turbo`** (mod√®les plus anciens) :
- **RPM** : G√©n√©ralement 3,500 requ√™tes par minute
- **TPM** : G√©n√©ralement 90,000 tokens par minute
- **RPD** : Variable selon votre plan

**Pour `gpt-4o`** :
- **RPM** : G√©n√©ralement 5,000 requ√™tes par minute
- **TPM** : G√©n√©ralement 20,000 tokens par minute
- **RPD** : Variable selon votre plan

#### Comment interpr√©ter les limites

- **TPM (Tokens Per Minute)** : Limite de tokens que vous pouvez utiliser par minute. Si d√©pass√© ‚Üí erreur 429
- **RPM (Requests Per Minute)** : Limite de requ√™tes API que vous pouvez faire par minute. Si d√©pass√© ‚Üí erreur 429
- **TPD (Tokens Per Day)** : Limite de tokens par jour pour les Batch Queues. Si d√©pass√© ‚Üí erreur 429

**Important** :
- Les limites sont appliqu√©es par minute (TPM, RPM) ou par jour (TPD)
- Si vous d√©passez une limite, vous recevrez une erreur 429 (Too Many Requests)
- Les limites peuvent varier selon la version sp√©cifique du mod√®le (v√©rifiez le tableau d√©taill√©)
- Votre "Usage tier" d√©termine vos limites exactes

#### V√©rifier votre utilisation actuelle

1. Allez sur https://platform.openai.com/usage
2. Consultez les graphiques pour voir :
   - Votre utilisation par jour
   - Le nombre de requ√™tes
   - Le nombre de tokens utilis√©s
   - Les co√ªts associ√©s

## üîß Solutions techniques impl√©ment√©es

### 1. Syst√®me de retry automatique

Le service AI Matchmaker impl√©mente maintenant :
- **Retry automatique** : Jusqu'√† 2 tentatives suppl√©mentaires en cas d'erreur 429
- **Backoff exponentiel** : D√©lais d'attente de 1s, 2s, 4s entre les tentatives
- **Logs d√©taill√©s** : Pour suivre les tentatives et les erreurs

### 2. Fallback intelligent

Si l'API OpenAI est indisponible, le service :
- G√©n√®re automatiquement des suggestions bas√©es sur les donn√©es disponibles
- D√©tecte l'intention de l'utilisateur (activit√©s vs partenaires)
- Fournit toujours une r√©ponse utile m√™me sans IA

## üìä Monitoring de l'utilisation

### V√©rifier l'utilisation actuelle

1. Allez sur https://platform.openai.com/usage
2. Consultez :
   - **Usage par jour** : Nombre de requ√™tes et tokens utilis√©s
   - **Co√ªts** : Montant d√©pens√©
   - **Graphiques** : Tendances d'utilisation

### Optimiser les co√ªts

1. **Utiliser `gpt-3.5-turbo`** (d√©j√† configur√©) :
   - Beaucoup moins cher que `gpt-4`
   - Suffisant pour la plupart des cas d'usage
   - Plus rapide

2. **R√©duire `max_tokens`** :
   - Actuellement : 1000 tokens
   - Vous pouvez r√©duire √† 500-750 pour √©conomiser

3. **Impl√©menter un cache** :
   - Mettre en cache les r√©ponses pour les requ√™tes similaires
   - R√©duit les appels API r√©p√©t√©s

## üöÄ Am√©liorations futures possibles

### 1. Syst√®me de cache

```typescript
// Exemple d'impl√©mentation de cache
private cache = new Map<string, ChatResponseDto>();

// Utiliser un hash du message pour le cache
const cacheKey = hashMessage(chatRequest.message);
if (this.cache.has(cacheKey)) {
  return this.cache.get(cacheKey);
}
```

### 2. Rate limiting c√¥t√© backend

- Limiter le nombre de requ√™tes par utilisateur
- Impl√©menter un syst√®me de queue pour les requ√™tes

### 3. Alternative gratuite

- **Google Gemini API** : Gratuit jusqu'√† un certain quota
- **Hugging Face Inference API** : Mod√®les open-source gratuits
- **Anthropic Claude API** : Alternative √† OpenAI

## üìù Configuration actuelle

### Variables d'environnement

```env
# Mod√®le OpenAI (par d√©faut: gpt-3.5-turbo)
OPENAI_MODEL=gpt-3.5-turbo

# Cl√© API OpenAI
OPENAI_API_KEY=sk-proj-...
```

### Mod√®les disponibles

- `gpt-3.5-turbo` : **Recommand√©** - Rapide, √©conomique, accessible
- `gpt-4o` : Plus puissant mais plus cher
- `gpt-4-turbo` : Version optimis√©e de GPT-4
- `gpt-4o-mini` : Version mini de GPT-4o

## ‚ö†Ô∏è Important

1. **Ne jamais commiter la cl√© API** : Elle est dans les variables d'environnement
2. **Surveiller les co√ªts** : V√©rifiez r√©guli√®rement votre utilisation
3. **Utiliser le fallback** : Le service fonctionne m√™me si l'API est indisponible
4. **Optimiser les prompts** : Des prompts plus courts = moins de tokens = moins cher

## üîó Liens utiles

- **Facturation** : https://platform.openai.com/account/billing
- **Utilisation** : https://platform.openai.com/usage
- **Documentation API** : https://platform.openai.com/docs
- **Prix** : https://openai.com/api/pricing/
- **Limites** : https://platform.openai.com/docs/guides/rate-limits

